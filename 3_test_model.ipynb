{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Create ConnectX Environment"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading environment lux_ai_2022 failed: No module named 'vec_noise'\n"]}],"source":["import torch as T\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","from typing import Callable\n","\n","from ipywidgets import IntProgress\n","from IPython.display import display\n","\n","from kaggle_environments import evaluate, make, utils\n","from kaggle_environments.utils import Struct\n","\n","env = make(\"connectx\", debug=True)\n","env.render()"]},{"cell_type":"markdown","metadata":{},"source":["# Create agents"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["class AgentModel(nn.Module):\n","    \n","    def __init__(self):\n","        super(AgentModel, self).__init__()\n","        self.input_layer = nn.Linear(44, 128)\n","        self.middle_layer = nn.Linear(128, 128) \n","        self.output_layer = nn.Linear(128, 7) \n","        self.optimizer = optim.Adam(self.parameters(), lr=0.003)\n","        self.loss = nn.MSELoss()\n","        self.device = 'cpu'\n","        self.to(self.device)\n","\n","    def load_action(\n","        self,\n","        observation_input: list,\n","        board: list,\n","        rows: int,\n","        cols: int\n","    ) -> int:\n","        state = T.tensor(observation_input, dtype=T.float32).to('cpu')\n","        actions = self.forward(state)\n","\n","        board = np.array(board).reshape(rows, cols).T\n","        base_actions_list: list = actions.tolist()\n","        final_actions_list: list = actions.tolist()      \n","        actions_dict = {k: v for k, v in zip(base_actions_list, range(len(base_actions_list)))}\n","        \n","        for i in range(cols):\n","            if board[i][0]:\n","                final_actions_list.remove(base_actions_list[i])\n","        \n","        if len(final_actions_list):\n","            action = actions_dict[max(final_actions_list)]\n","        else:\n","            action = 0\n","        \n","        return action\n","\n","    def forward(self, state) -> T.tensor:\n","        x = F.relu(self.input_layer(state))\n","        x = F.relu(self.middle_layer(x))\n","        actions = self.output_layer(x)\n","\n","        return actions"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def random_agent(observation: Struct, configuration: Struct = None) -> int:\n","    '''Random agent'''\n","    from random import choice\n","    return choice([c for c in range(7) if observation.board[c] == 0])"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def base_agent(observation: Struct, configuration: Struct = None) -> int:\n","    weights_path = './data/base_weights'\n","    model = AgentModel()\n","    model.load_state_dict(T.load(weights_path))\n","\n","    if configuration is None:\n","        rows=6\n","        cols=7\n","    else:\n","        rows = configuration.rows\n","        cols = configuration.cols\n","        \n","    observation_input= observation.board + [observation.step, observation.mark]\n","    action = model.load_action(observation_input, observation.board, rows, cols)\n","\n","    return action"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def extra_reward_agent(observation: Struct, configuration: Struct = None) -> int:\n","    weights_path = './data/punishment_and_extra_rewards_weights'\n","    model = AgentModel()\n","    model.load_state_dict(T.load(weights_path))\n","\n","    if configuration is None:\n","        rows=6\n","        cols=7\n","    else:\n","        rows = configuration.rows\n","        cols = configuration.cols\n","        \n","    observation_input= observation.board + [observation.step, observation.mark]\n","    action = model.load_action(observation_input, observation.board, rows, cols)\n","\n","    return action"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def extra_punish_agent(observation: Struct, configuration: Struct = None) -> int:\n","    weights_path = './data/punishment_minus_3_weights'\n","    model = AgentModel()\n","    model.load_state_dict(T.load(weights_path))\n","\n","    if configuration is None:\n","        rows=6\n","        cols=7\n","    else:\n","        rows = configuration.rows\n","        cols = configuration.cols\n","        \n","    observation_input= observation.board + [observation.step, observation.mark]\n","    action = model.load_action(observation_input, observation.board, rows, cols)\n","\n","    return action"]},{"cell_type":"markdown","metadata":{},"source":["# Test your Agent"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def test_agent(\n","    n_games: int,\n","    trainer: Struct,\n","    agent: Callable\n",") -> float:\n","    wins = 0\n","    # f = IntProgress(min=0, max=n_games, width=100) # instantiate the bar\n","    # display(f)\n","\n","    for i in range(n_games):\n","        done  = False\n","        observation = trainer.reset()\n","        \n","        while not done:\n","            action = agent(observation)\n","            observation, reward, done, info = trainer.step(int(action))\n","            wins = wins + 1 if reward == 1 else wins\n","        \n","        # f.value += 1\n","    \n","    return wins/n_games"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.136\n"]}],"source":["env = make(\"connectx\", debug=True)\n","env.render()\n","\n","n_games = 1000\n","trainer = env.train([None, \"negamax\"])\n","wins_ratio = test_agent(n_games, trainer, extra_punish_agent)\n","\n","print(wins_ratio)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.848\n"]}],"source":["env = make(\"connectx\", debug=True)\n","env.render()\n","\n","n_games = 1000\n","trainer = env.train([None, \"random\"])\n","wins_ratio = test_agent(n_games, trainer, extra_punish_agent)\n","\n","print(wins_ratio)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.2 ('.env': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"76fe824bd98976dc29cd20b9fe5d36c656768397f3d319104e5428f9aaab9bff"}}},"nbformat":4,"nbformat_minor":4}
