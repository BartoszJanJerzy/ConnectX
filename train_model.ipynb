{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch as T\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","from typing import Iterable\n","import gym\n","\n","from kaggle_environments import evaluate, make, utils\n","from kaggle_environments.utils import Struct"]},{"cell_type":"markdown","metadata":{},"source":["# Model architecture"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DQN(nn.Module):\n","\n","    def __init__(\n","        self,\n","        learning_rate: float,\n","        input_dims: Iterable,\n","        middle_dims: int,\n","        output_dims: int,\n","        n_actions: int\n","    ):\n","        super(DQN, self).__init__()\n","        self.input_layer = nn.Linear(*input_dims, middle_dims)\n","        self.middle_layer = nn.Linear(middle_dims, output_dims) \n","        self.output_layer = nn.Linear(output_dims, n_actions) \n","        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n","        self.loss = nn.MSELoss()\n","        self.device = 'cpu'\n","        self.to(self.device)\n","\n","    def forward(self, state):\n","        x = F.relu(self.input_layer(state))\n","        x = F.relu(self.middle_layer(x))\n","        actions = self.output_layer(x)\n","\n","        return actions"]},{"cell_type":"markdown","metadata":{},"source":["# Agent"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Agent:\n","\n","    ROWS = 6\n","    COLS = 7\n","\n","    def __init__(\n","        self,\n","        gamma: float,\n","        epsilon: float,\n","        learning_rate: float,\n","        input_dims: Iterable,\n","        batch_size: int,\n","        n_actions: int,\n","        max_mem_size: int = 100000\n","    ):\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","        self.learning_rate = learning_rate\n","        self.action_space = [i for i in range(n_actions)]\n","        self.mem_size = max_mem_size\n","        self.batch_size = batch_size\n","        self.mem_counter = 0\n","\n","        self.Q_eval = DQN(\n","            learning_rate=self.learning_rate,\n","            input_dims=input_dims,\n","            middle_dims=32,\n","            output_dims=32,\n","            n_actions=n_actions\n","        )\n","        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n","        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n","        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n","        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n","        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n","\n","    def store_transition(self, state, action, reward, state_, done):\n","        index = self.mem_counter % self.mem_size\n","        self.state_memory[index] = state\n","        self.new_state_memory[index] = state_\n","        self.reward_memory[index] = reward\n","        self.action_memory[index] = action\n","        self.terminal_memory[index] = done\n","\n","        self.mem_counter += 1\n","    \n","    def choose_action(self, observation: Struct, board: list) -> int:\n","        state = T.tensor([observation], dtype=T.float32).to(self.Q_eval.device)\n","        actions = self.Q_eval.forward(state)\n","        action = self._load_available_action(actions, board)\n","        return action\n","    \n","    def learn(self):\n","        if self.mem_counter < self.batch_size:\n","            return \n","        \n","        self.Q_eval.optimizer.zero_grad()\n","\n","        max_mem = min(self.mem_counter, self.mem_size)\n","        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n","        batch_index = np.arange(self.batch_size, dtype=np.int32)\n","\n","        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n","        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n","        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n","        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n","        action_batch = self.action_memory[batch]\n","\n","        # loss function\n","        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n","        q_next = self.Q_eval.forward(new_state_batch)\n","        q_next['terminal_batch'] = 0.0\n","        \n","        q_target = reward_batch + self.gamma + T.max(q_next, dim=1)[0]\n","        \n","        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n","        loss.backward()\n","        self.Q_eval.optimizer.step()\n","\n","    def _load_available_action(self, actions, board) -> int:\n","        board = np.array(board).reshape(self.ROWS, self.COLS).T\n","\n","        actions_list: list = actions.tolist()[0]        \n","        actions_dict = {k: v for k, v in zip(actions_list, range(len(actions_list)))}\n","\n","        for i in range(self.COLS):\n","            if board[i][0]:\n","                actions_list.remove(max(actions_list))\n","        \n","        if len(actions_list):\n","            action = actions_dict[max(actions_list)]\n","        else:\n","            action = 0\n","        \n","        return action\n"]},{"cell_type":"markdown","metadata":{},"source":["# Train agent"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_games = 1\n","env = make(\"connectx\", debug=True)\n","env.render()\n","trainer = env.train([None, \"random\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["agent = Agent(\n","    gamma=0.99,\n","    epsilon=1.0,\n","    learning_rate=0.003,\n","    input_dims=[44],\n","    batch_size=64,\n","    n_actions=7\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_model_input(observation: Struct) -> list:\n","    return observation.board + [observation.step, observation.mark]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(n_games):\n","    done  = False\n","    observation = trainer.reset()\n","    \n","    while not done:\n","        model_input = get_model_input(observation)\n","        action = agent.choose_action(model_input, observation.board)\n","        observation_, reward, done, info = trainer.step(int(action))\n","\n","        model_next_input = get_model_input(observation_)\n","        agent.store_transition(model_input, action, reward, model_next_input, done)\n","        agent.learn()\n","        observation = observation_\n","        \n","    env.render(mode=\"ipython\", width=500, height=500, header=False, controls=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.2 ('.env': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"76fe824bd98976dc29cd20b9fe5d36c656768397f3d319104e5428f9aaab9bff"}}},"nbformat":4,"nbformat_minor":4}
