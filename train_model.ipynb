{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch as T\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","from typing import Iterable\n","import gym\n","from datetime import datetime\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","import os\n","\n","from kaggle_environments import evaluate, make, utils\n","from kaggle_environments.utils import Struct"]},{"cell_type":"markdown","metadata":{},"source":["# Model architecture"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DQN(nn.Module):\n","\n","    def __init__(\n","        self,\n","        learning_rate: float,\n","        input_dims: Iterable,\n","        middle_dims: int,\n","        output_dims: int,\n","        n_actions: int\n","    ):\n","        super(DQN, self).__init__()\n","        self.input_layer = nn.Linear(*input_dims, middle_dims)\n","        self.middle_layer = nn.Linear(middle_dims, output_dims) \n","        self.output_layer = nn.Linear(output_dims, n_actions) \n","        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n","        self.loss = nn.MSELoss()\n","        self.device = 'cpu'\n","        self.to(self.device)\n","\n","    def forward(self, state):\n","        x = F.relu(self.input_layer(state))\n","        x = F.relu(self.middle_layer(x))\n","        actions = self.output_layer(x)\n","\n","        return actions"]},{"cell_type":"markdown","metadata":{},"source":["# Agent"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Agent:\n","\n","    ROWS = 6\n","    COLS = 7\n","\n","    def __init__(\n","        self,\n","        gamma: float,\n","        learning_rate: float,\n","        input_dims: Iterable,\n","        batch_size: int,\n","        n_actions: int,\n","        max_mem_size: int = 100000\n","    ):\n","        self.gamma = gamma\n","        self.learning_rate = learning_rate\n","        self.action_space = [i for i in range(n_actions)]\n","        self.mem_size = max_mem_size\n","        self.batch_size = batch_size\n","        self.mem_counter = 0\n","\n","        self.Q_eval = DQN(\n","            learning_rate=self.learning_rate,\n","            input_dims=input_dims,\n","            middle_dims=128,\n","            output_dims=128,\n","            n_actions=n_actions\n","        )\n","        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n","        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n","        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n","        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n","        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n","\n","    def store_transition(self, state, action, reward, state_, done):\n","        index = self.mem_counter % self.mem_size\n","        self.state_memory[index] = state\n","        self.new_state_memory[index] = state_\n","        self.reward_memory[index] = reward\n","        self.action_memory[index] = action\n","        self.terminal_memory[index] = done\n","\n","        self.mem_counter += 1\n","    \n","    def choose_action(self, observation: Struct, board: list) -> int:\n","        state = T.tensor([observation], dtype=T.float32).to(self.Q_eval.device)\n","        actions = self.Q_eval.forward(state)\n","        action = self._load_available_action(actions, board)\n","        return action\n","    \n","    def learn(self):\n","        if self.mem_counter < self.batch_size:\n","            return \n","        \n","        self.Q_eval.optimizer.zero_grad()\n","\n","        max_mem = min(self.mem_counter, self.mem_size)\n","        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n","        batch_index = np.arange(self.batch_size, dtype=np.int32)\n","\n","        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n","        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n","        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n","        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n","        action_batch = self.action_memory[batch]\n","\n","        # loss function\n","        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n","        q_next = self.Q_eval.forward(new_state_batch)\n","        # q_next['terminal_batch'] = 0\n","        \n","        q_target = reward_batch + self.gamma + T.max(q_next, dim=1)[0]\n","        \n","        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n","        loss.backward()\n","        self.Q_eval.optimizer.step()\n","\n","    def _load_available_action(self, actions, board) -> int:\n","        board = np.array(board).reshape(self.ROWS, self.COLS).T\n","\n","        base_actions_list: list = actions.tolist()[0]    \n","        final_actions_list: list = actions.tolist()[0]        \n","        actions_dict = {k: v for k, v in zip(base_actions_list, range(len(base_actions_list)))}\n","\n","        for i in range(self.COLS):\n","            if board[i][0]:\n","                final_actions_list.remove(base_actions_list[i])\n","        \n","        if len(final_actions_list):\n","            action = actions_dict[max(final_actions_list)]\n","        else:\n","            action = 0\n","        \n","        return action\n"]},{"cell_type":"markdown","metadata":{},"source":["# Train agent"]},{"cell_type":"markdown","metadata":{},"source":["## Functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_model_input(observation: Struct) -> list:\n","    return observation.board + [observation.step, observation.mark]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def count_reward(\n","    n_rounds_without_loose: int,\n","    baseline_reward: int\n",") -> int:\n","    reward = 0\n","    \n","    if not n_rounds_without_loose % 5:\n","        reward += 1\n","    \n","    if not n_rounds_without_loose % 10:\n","        reward += 1\n","\n","    if baseline_reward == 1:\n","        reward = 5\n","    \n","    return reward"]},{"cell_type":"markdown","metadata":{},"source":["## Training parameters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_games = 10000\n","env = make(\"connectx\", debug=True)\n","env.render()\n","trainer = env.train([None, \"negamax\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["agent = Agent(\n","    gamma=0.99,\n","    learning_rate=0.003,\n","    input_dims=[44],\n","    batch_size=64,\n","    n_actions=7\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Play the game!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["final_rewards = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["start = datetime.now()\n","\n","for i in range(n_games):\n","    done  = False\n","    observation = trainer.reset()\n","    n_rounds_without_loose = 0\n","    rewards = []\n","\n","    while not done:\n","        model_input = get_model_input(observation)\n","        action = agent.choose_action(model_input, observation.board)\n","        observation_, reward, done, info = trainer.step(int(action))\n","\n","        if reward != -1:\n","            n_rounds_without_loose += 1\n","        reward = count_reward(n_rounds_without_loose, reward)\n","        rewards.append(reward)\n","\n","        model_next_input = get_model_input(observation_)\n","        agent.store_transition(model_input, action, reward, model_next_input, done)\n","        agent.learn()\n","        observation = observation_\n","    \n","    final_rewards.append(max(rewards))\n","    \n","stop = datetime.now()\n","seconds_spent = (stop - start).seconds\n","\n","env.render(mode=\"ipython\", width=500, height=500, header=False, controls=False)"]},{"cell_type":"markdown","metadata":{},"source":["# See games history"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def count_means(final_rewards: list) -> list:\n","    data_to_mean = []\n","    data = []\n","\n","    for i in range(len(final_rewards)):\n","        if i % 10:\n","            data_to_mean.append(final_rewards[i])\n","        else:\n","            current_mean = round(np.mean(data_to_mean), 2)\n","            data += [current_mean]*10\n","            data_to_mean = []\n","    \n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_cumulative_wins(final_rewards: list) -> list:\n","    data = []\n","    wins = 0\n","\n","    for reward in final_rewards:\n","        if reward == 5:\n","            wins += 1\n","        \n","        data.append(wins)\n","    \n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def show_history_plot(final_rewards: list):\n","    x = [i for i in range(len(final_rewards))]\n","    moving_means = count_means(final_rewards)\n","    cumulative_wins = load_cumulative_wins(final_rewards)\n","\n","    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n","\n","    fig.add_trace(go.Scattergl(\n","        name='Rewards',\n","        x=x,\n","        y=final_rewards,\n","        mode='lines',\n","        opacity=0.3\n","    ))\n","    fig.add_trace(go.Scattergl(\n","        name='Moving mean for 10 games',\n","        x=x,\n","        y=moving_means,\n","        mode='lines',\n","        marker_color='orange'\n","    )) \n","    fig.add_trace(go.Scattergl(\n","        name='Cumulative wins num',\n","        x=x,\n","        y=cumulative_wins,\n","        # mode='lines',\n","        marker_color='rgb(0, 190, 0)'\n","    ), secondary_y=True) \n","\n","    fig.update_layout(\n","        title='0: loose<br>1 or 2: 5 or 10 rounds without loose<br>5: win',\n","        xaxis_title='Games',\n","        yaxis_title='Rewards',\n","        yaxis2=dict(\n","            title=\"Cumulative wins\",\n","            titlefont=dict(color='rgb(0, 190, 0)'),\n","            tickfont=dict(color='rgb(0, 190, 0)')\n","        ),\n","    )\n","\n","    fig.show(renderer='notebook')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["show_history_plot(final_rewards)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.mean(final_rewards[:20])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.mean(final_rewards[-20:])"]},{"cell_type":"markdown","metadata":{},"source":["# Save weights"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["save_path = os.path.join(os.getcwd(), 'data', 'first_models_weights')\n","T.save(agent.Q_eval.state_dict(), save_path)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.2 ('.env': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"vscode":{"interpreter":{"hash":"76fe824bd98976dc29cd20b9fe5d36c656768397f3d319104e5428f9aaab9bff"}}},"nbformat":4,"nbformat_minor":4}
